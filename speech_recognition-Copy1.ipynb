{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4afcd2c5-be53-4f5e-bcd2-56bb75bc8d5b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Option 1: STT process Using LibriSpeech (Torch Audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e64a281-6c7d-46a5-80f4-ca94a4c165ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utterance: HE HOPED THERE WOULD BE STEW FOR DINNER TURNIPS AND CARROTS AND BRUISED POTATOES AND FAT MUTTON PIECES TO BE LADLED OUT IN THICK PEPPERED FLOUR FATTENED SAUCE\n",
      "Sample Rate: 16000 Hz\n",
      "Utterance: STUFF IT INTO YOU HIS BELLY COUNSELLED HIM\n",
      "Sample Rate: 16000 Hz\n",
      "Utterance: AFTER EARLY NIGHTFALL THE YELLOW LAMPS WOULD LIGHT UP HERE AND THERE THE SQUALID QUARTER OF THE BROTHELS\n",
      "Sample Rate: 16000 Hz\n",
      "Utterance: HELLO BERTIE ANY GOOD IN YOUR MIND\n",
      "Sample Rate: 16000 Hz\n",
      "Utterance: NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD NIGHT HUSBAND\n",
      "Sample Rate: 16000 Hz\n",
      "Utterance: THE MUSIC CAME NEARER AND HE RECALLED THE WORDS THE WORDS OF SHELLEY'S FRAGMENT UPON THE MOON WANDERING COMPANIONLESS PALE FOR WEARINESS\n",
      "Sample Rate: 16000 Hz\n",
      "Utterance: THE DULL LIGHT FELL MORE FAINTLY UPON THE PAGE WHEREON ANOTHER EQUATION BEGAN TO UNFOLD ITSELF SLOWLY AND TO SPREAD ABROAD ITS WIDENING TAIL\n",
      "Sample Rate: 16000 Hz\n",
      "Utterance: A COLD LUCID INDIFFERENCE REIGNED IN HIS SOUL\n",
      "Sample Rate: 16000 Hz\n",
      "Utterance: THE CHAOS IN WHICH HIS ARDOUR EXTINGUISHED ITSELF WAS A COLD INDIFFERENT KNOWLEDGE OF HIMSELF\n",
      "Sample Rate: 16000 Hz\n",
      "Utterance: AT MOST BY AN ALMS GIVEN TO A BEGGAR WHOSE BLESSING HE FLED FROM HE MIGHT HOPE WEARILY TO WIN FOR HIMSELF SOME MEASURE OF ACTUAL GRACE\n",
      "Sample Rate: 16000 Hz\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "\n",
    "# Load a sample audio file from LibriSpeech\n",
    "datasets = torchaudio.datasets.LIBRISPEECH(\".\", url=\"test-clean\", download=True)\n",
    "\n",
    "key = 0\n",
    "for dataset in datasets:  \n",
    "    if ( key < 10):\n",
    "        # Access the first audio sample in the dataset\n",
    "        waveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id = dataset\n",
    "        \n",
    "        # Save the audio to a WAV file\n",
    "        torchaudio.save(\"librispeech_sample.wav\", waveform, sample_rate)\n",
    "        key += 1\n",
    "        \n",
    "        print(f\"Utterance: {utterance}\")\n",
    "        print(f\"Sample Rate: {sample_rate} Hz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40158ee-486b-441e-b454-ca1fba156f98",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Option 2: STT Process Using Cloud APIs (Google Speech-to-Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62ce505d-1ab4-4b23-8895-855d39ccf26e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Audio file could not be read as PCM WAV, AIFF/AIFF-C, or Native FLAC; check if file is corrupted or in another format",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\stt-erp\\lib\\site-packages\\speech_recognition\\__init__.py:233\u001b[0m, in \u001b[0;36mAudioFile.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;66;03m# attempt to read the file as WAV\u001b[39;00m\n\u001b[1;32m--> 233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_reader \u001b[38;5;241m=\u001b[39m \u001b[43mwave\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilename_or_fileobject\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlittle_endian \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# RIFF WAV is a little-endian format (most ``audioop`` operations assume that the frames are stored in little-endian form)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stt-erp\\lib\\wave.py:509\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(f, mode)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mWave_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stt-erp\\lib\\wave.py:163\u001b[0m, in \u001b[0;36mWave_read.__init__\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitfp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stt-erp\\lib\\wave.py:143\u001b[0m, in \u001b[0;36mWave_read.initfp\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunkname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfmt \u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_fmt_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fmt_chunk_read \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stt-erp\\lib\\wave.py:268\u001b[0m, in \u001b[0;36mWave_read._read_fmt_chunk\u001b[1;34m(self, chunk)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Error(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munknown format: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (wFormatTag,))\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nchannels:\n",
      "\u001b[1;31mError\u001b[0m: unknown format: 3",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\stt-erp\\lib\\site-packages\\speech_recognition\\__init__.py:238\u001b[0m, in \u001b[0;36mAudioFile.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;66;03m# attempt to read the file as AIFF\u001b[39;00m\n\u001b[1;32m--> 238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_reader \u001b[38;5;241m=\u001b[39m \u001b[43maifc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilename_or_fileobject\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlittle_endian \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# AIFF is a big-endian format\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stt-erp\\lib\\aifc.py:917\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(f, mode)\u001b[0m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAifc_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stt-erp\\lib\\aifc.py:352\u001b[0m, in \u001b[0;36mAifc_read.__init__\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 352\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitfp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_object\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stt-erp\\lib\\aifc.py:316\u001b[0m, in \u001b[0;36mAifc_read.initfp\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mgetname() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFORM\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 316\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Error(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile does not start with FORM id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    317\u001b[0m formdata \u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[1;31mError\u001b[0m: file does not start with FORM id",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\stt-erp\\lib\\site-packages\\speech_recognition\\__init__.py:264\u001b[0m, in \u001b[0;36mAudioFile.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_reader \u001b[38;5;241m=\u001b[39m \u001b[43maifc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43maiff_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (aifc\u001b[38;5;241m.\u001b[39mError, \u001b[38;5;167;01mEOFError\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stt-erp\\lib\\aifc.py:917\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(f, mode)\u001b[0m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAifc_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stt-erp\\lib\\aifc.py:358\u001b[0m, in \u001b[0;36mAifc_read.__init__\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;66;03m# assume it is an open file object already\u001b[39;00m\n\u001b[1;32m--> 358\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitfp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stt-erp\\lib\\aifc.py:314\u001b[0m, in \u001b[0;36mAifc_read.initfp\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m file\n\u001b[1;32m--> 314\u001b[0m chunk \u001b[38;5;241m=\u001b[39m \u001b[43mChunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mgetname() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFORM\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stt-erp\\lib\\chunk.py:63\u001b[0m, in \u001b[0;36mChunk.__init__\u001b[1;34m(self, file, align, bigendian, inclheader)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunkname) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mEOFError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m recognizer \u001b[38;5;241m=\u001b[39m sr\u001b[38;5;241m.\u001b[39mRecognizer()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load the audio file\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sr\u001b[38;5;241m.\u001b[39mAudioFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibrispeech_sample.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m source:\n\u001b[0;32m      8\u001b[0m     audio_data \u001b[38;5;241m=\u001b[39m recognizer\u001b[38;5;241m.\u001b[39mrecord(source)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Convert speech to text\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stt-erp\\lib\\site-packages\\speech_recognition\\__init__.py:266\u001b[0m, in \u001b[0;36mAudioFile.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    264\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_reader \u001b[38;5;241m=\u001b[39m aifc\u001b[38;5;241m.\u001b[39mopen(aiff_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    265\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m (aifc\u001b[38;5;241m.\u001b[39mError, \u001b[38;5;167;01mEOFError\u001b[39;00m):\n\u001b[1;32m--> 266\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAudio file could not be read as PCM WAV, AIFF/AIFF-C, or Native FLAC; check if file is corrupted or in another format\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    267\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlittle_endian \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# AIFF is a big-endian format\u001b[39;00m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_reader\u001b[38;5;241m.\u001b[39mgetnchannels() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAudio must be mono or stereo\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: Audio file could not be read as PCM WAV, AIFF/AIFF-C, or Native FLAC; check if file is corrupted or in another format"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "# Initialize recognizer\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Load the audio file\n",
    "with sr.AudioFile(\"librispeech_sample.wav\") as source:\n",
    "    audio_data = recognizer.record(source)\n",
    "\n",
    "# Convert speech to text\n",
    "try:\n",
    "    text = recognizer.recognize_google(audio_data)\n",
    "    print(\"Transcribed Text:\", text)\n",
    "except Exception as e:\n",
    "    print(\"Error:\", str(e))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b881695e-b681-4aae-86a7-9288e683343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "# Convert audio to PCM WAV format\n",
    "audio = AudioSegment.from_file(\"librispeech_sample.wav\")\n",
    "audio = audio.set_frame_rate(16000).set_channels(1)\n",
    "audio.export(\"output.wav\", format=\"wav\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5d1389-ac4d-4494-8a97-d80880dc2ec9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Option 3: STT Process Using OpenAI Whisper (Local Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ca95f3a-1a67-4d9b-97bb-edbeda0cb47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\komal\\anaconda3\\envs\\stt-erp\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ffmpeg was not found but is required to load audio files from filename",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\stt-erp\\lib\\site-packages\\transformers\\pipelines\\audio_utils.py:34\u001b[0m, in \u001b[0;36mffmpeg_read\u001b[1;34m(bpayload, sampling_rate)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mffmpeg_command\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m ffmpeg_process:\n\u001b[0;32m     35\u001b[0m         output_stream \u001b[38;5;241m=\u001b[39m ffmpeg_process\u001b[38;5;241m.\u001b[39mcommunicate(bpayload)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stt-erp\\lib\\subprocess.py:951\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[0;32m    948\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m    949\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m--> 951\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stt-erp\\lib\\subprocess.py:1436\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1435\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1436\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1437\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1438\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1439\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1440\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1441\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1442\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1443\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1444\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1446\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1449\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1450\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m speech_to_text \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautomatic-speech-recognition\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai/whisper-small\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Perform STT\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mspeech_to_text\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlibrispeech_sample.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranscribed Text:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stt-erp\\lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py:283\u001b[0m, in \u001b[0;36mAutomaticSpeechRecognitionPipeline.__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    224\u001b[0m     inputs: Union[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    226\u001b[0m ):\n\u001b[0;32m    227\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03m    Transcribe the audio sequence(s) given as inputs to text. See the [`AutomaticSpeechRecognitionPipeline`]\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03m    documentation for more information.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;124;03m                `\"\".join(chunk[\"text\"] for chunk in output[\"chunks\"])`.\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stt-erp\\lib\\site-packages\\transformers\\pipelines\\base.py:1293\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ChunkPipeline):\n\u001b[1;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stt-erp\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stt-erp\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py:269\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[1;32m--> 269\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    271\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stt-erp\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stt-erp\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stt-erp\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:33\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_iter\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stt-erp\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py:186\u001b[0m, in \u001b[0;36mPipelineChunkIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubiterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;66;03m# Try to return next item\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubiterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;66;03m# When a preprocess iterator ends, we can start lookig at the next item\u001b[39;00m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;66;03m# ChunkIterator will keep feeding until ALL elements of iterator\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;66;03m# Another way to look at it, is we're basically flattening lists of lists\u001b[39;00m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;66;03m# into a single list, but with generators\u001b[39;00m\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubiterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stt-erp\\lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py:364\u001b[0m, in \u001b[0;36mAutomaticSpeechRecognitionPipeline.preprocess\u001b[1;34m(self, inputs, chunk_length_s, stride_length_s)\u001b[0m\n\u001b[0;32m    361\u001b[0m             inputs \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m--> 364\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43mffmpeg_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    366\u001b[0m stride \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    367\u001b[0m extra \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stt-erp\\lib\\site-packages\\transformers\\pipelines\\audio_utils.py:37\u001b[0m, in \u001b[0;36mffmpeg_read\u001b[1;34m(bpayload, sampling_rate)\u001b[0m\n\u001b[0;32m     35\u001b[0m         output_stream \u001b[38;5;241m=\u001b[39m ffmpeg_process\u001b[38;5;241m.\u001b[39mcommunicate(bpayload)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m---> 37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mffmpeg was not found but is required to load audio files from filename\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m\n\u001b[0;32m     38\u001b[0m out_bytes \u001b[38;5;241m=\u001b[39m output_stream[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     39\u001b[0m audio \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(out_bytes, np\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[1;31mValueError\u001b[0m: ffmpeg was not found but is required to load audio files from filename"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load Whisper model\n",
    "speech_to_text = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-small\")\n",
    "\n",
    "# Perform STT\n",
    "result = speech_to_text(\"librispeech_sample.wav\")\n",
    "print(\"Transcribed Text:\", result['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa00228-44c9-4dda-8f2b-dccc27e367df",
   "metadata": {},
   "source": [
    "### Option 3: STT Process Using Wav2Vec 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bce22844-0921-49e0-bcc1-d8484dcf77e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribed Text: I LOVE THEE WITH TE LOVE I SEEMED TO LOSE WITH MY LOST SAINTS I LOVE THEE WITH THE BREATH SMILES TEARS OF ALL MY LIFE AND IF GOD CHOOSE I SHALL BUT LOVE THEE BETTER AFTER DEATH\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import torch\n",
    "import librosa\n",
    "\n",
    "# Load model and processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# Load audio and preprocess\n",
    "audio_input, sample_rate = librosa.load(\"librispeech_sample.wav\", sr=16000)\n",
    "input_values = processor(audio_input, sampling_rate=16000, return_tensors=\"pt\").input_values\n",
    "\n",
    "# Perform STT\n",
    "logits = model(input_values).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.decode(predicted_ids[0])\n",
    "print(\"Transcribed Text:\", transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeabc8c9-8d85-4ed8-86aa-5165ce191847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I LOVE THEE WITH TE LOVE I SEEMED TO LOSE WITH MY LOST SAINTS I LOVE THEE WITH THE BREATH SMILES TEARS OF ALL MY LIFE AND IF GOD CHOOSE I SHALL BUT LOVE THEE BETTER AFTER DEATH']\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import torch\n",
    "\n",
    "# Load model and processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# Process input audio\n",
    "inputs = processor(audio_input, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "logits = model(inputs.input_values).logits\n",
    "\n",
    "# Decode predictions\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d0cb67-8563-4d65-a83f-a83dc84d0aa8",
   "metadata": {},
   "source": [
    "### Feature Extraction Using Librosa Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e4d16ddb-4c50-4f15-acae-edec3f938f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features shape: (2620, 33)\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "\n",
    "# Function to extract features\n",
    "def extract_features(file_path):\n",
    "    audio, sample_rate = librosa.load(file_path, sr=16000)\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=13).T, axis=0)\n",
    "    chroma = np.mean(librosa.feature.chroma_stft(y=audio, sr=sample_rate).T, axis=0)\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(y=audio).T, axis=0)\n",
    "    spectral_contrast = np.mean(librosa.feature.spectral_contrast(y=audio, sr=sample_rate).T, axis=0)\n",
    "    return np.hstack([mfccs, chroma, zcr, spectral_contrast])\n",
    "\n",
    "# Load a sample audio file from LibriSpeech\n",
    "datasets = torchaudio.datasets.LIBRISPEECH(\".\", url=\"test-clean\", download=True)\n",
    "\n",
    "key = 0\n",
    "features = []\n",
    "file_path = \"librispeech_sample.wav\"\n",
    "for dataset in datasets:  \n",
    "    # Access the first audio sample in the dataset\n",
    "    waveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id = dataset\n",
    "    \n",
    "    # Save the audio to a WAV file\n",
    "    torchaudio.save(file_path, waveform, sample_rate)\n",
    "    key += 1\n",
    "\n",
    "    feature = extract_features(file_path)\n",
    "    features.append(feature) \n",
    "\n",
    "# Convert the feature list to a numpy array for further processing\n",
    "features_array = np.array(features)\n",
    "print(f\"Extracted features shape: {features_array.shape}\")\n",
    "\n",
    "# Convert np array into data freame \n",
    "features_df = pd.DataFrame(features_array)\n",
    "features_df.to_csv(\"features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "15ef7c8f-be5f-41bd-bb74-9b40acec923c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features shape: (2620, 33)\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "\n",
    "# Function to extract features from audio waveform\n",
    "def extract_features_from_waveform(waveform, sample_rate):\n",
    "    audio = waveform.numpy().flatten()  # Convert PyTorch tensor to numpy array\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=13).T, axis=0)\n",
    "    chroma = np.mean(librosa.feature.chroma_stft(y=audio, sr=sample_rate).T, axis=0)\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(y=audio).T, axis=0)\n",
    "    spectral_contrast = np.mean(librosa.feature.spectral_contrast(y=audio, sr=sample_rate).T, axis=0)\n",
    "    return np.hstack([mfccs, chroma, zcr, spectral_contrast])\n",
    "\n",
    "# Load a sample audio dataset from LibriSpeech\n",
    "datasets = torchaudio.datasets.LIBRISPEECH(\".\", url=\"test-clean\", download=True)\n",
    "\n",
    "# Extract features from first 10 utterances\n",
    "features = []\n",
    "key = 0\n",
    "for dataset in datasets:\n",
    "    # Access audio sample and its metadata\n",
    "    waveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id = dataset\n",
    "    \n",
    "    # Extract features directly from waveform\n",
    "    feature = extract_features_from_waveform(waveform, sample_rate)\n",
    "    features.append(feature)  # Append feature array for each utterance  \n",
    "\n",
    "# Convert the feature list to a numpy array for further processing\n",
    "features_array = np.array(features)\n",
    "print(f\"Extracted features shape: {features_array.shape}\")\n",
    "\n",
    "# Convert np array into data freame \n",
    "features_df = pd.DataFrame(features_array)\n",
    "features_df.to_csv(\"features.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2c906ec6-be8b-4097-8822-2e6e87485d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-300.538055</td>\n",
       "      <td>102.676567</td>\n",
       "      <td>-11.533751</td>\n",
       "      <td>34.515648</td>\n",
       "      <td>3.697970</td>\n",
       "      <td>3.740451</td>\n",
       "      <td>-7.779979</td>\n",
       "      <td>-5.030833</td>\n",
       "      <td>-1.161886</td>\n",
       "      <td>-0.948949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428673</td>\n",
       "      <td>0.454517</td>\n",
       "      <td>0.127497</td>\n",
       "      <td>19.646138</td>\n",
       "      <td>14.211446</td>\n",
       "      <td>16.890984</td>\n",
       "      <td>15.460212</td>\n",
       "      <td>17.686253</td>\n",
       "      <td>19.864507</td>\n",
       "      <td>14.851772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-364.247498</td>\n",
       "      <td>91.377350</td>\n",
       "      <td>4.104960</td>\n",
       "      <td>40.972740</td>\n",
       "      <td>3.855652</td>\n",
       "      <td>11.061490</td>\n",
       "      <td>-3.984364</td>\n",
       "      <td>-0.601738</td>\n",
       "      <td>4.034190</td>\n",
       "      <td>-2.416186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.511107</td>\n",
       "      <td>0.555132</td>\n",
       "      <td>0.107820</td>\n",
       "      <td>18.037720</td>\n",
       "      <td>13.369277</td>\n",
       "      <td>16.100759</td>\n",
       "      <td>14.565921</td>\n",
       "      <td>16.484215</td>\n",
       "      <td>19.391356</td>\n",
       "      <td>14.970760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-303.546387</td>\n",
       "      <td>130.277008</td>\n",
       "      <td>-17.590935</td>\n",
       "      <td>26.203365</td>\n",
       "      <td>-7.220541</td>\n",
       "      <td>2.721291</td>\n",
       "      <td>-7.697184</td>\n",
       "      <td>-8.856187</td>\n",
       "      <td>1.239684</td>\n",
       "      <td>-1.230602</td>\n",
       "      <td>...</td>\n",
       "      <td>0.466134</td>\n",
       "      <td>0.469798</td>\n",
       "      <td>0.087339</td>\n",
       "      <td>21.492927</td>\n",
       "      <td>15.680788</td>\n",
       "      <td>17.941670</td>\n",
       "      <td>17.006381</td>\n",
       "      <td>17.779363</td>\n",
       "      <td>20.308019</td>\n",
       "      <td>14.854105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-313.242920</td>\n",
       "      <td>110.683128</td>\n",
       "      <td>-20.646511</td>\n",
       "      <td>21.182190</td>\n",
       "      <td>-5.369611</td>\n",
       "      <td>-9.688619</td>\n",
       "      <td>-9.589869</td>\n",
       "      <td>-7.087545</td>\n",
       "      <td>-10.202554</td>\n",
       "      <td>-2.014711</td>\n",
       "      <td>...</td>\n",
       "      <td>0.426402</td>\n",
       "      <td>0.498014</td>\n",
       "      <td>0.065348</td>\n",
       "      <td>21.618438</td>\n",
       "      <td>17.300488</td>\n",
       "      <td>20.150371</td>\n",
       "      <td>17.723875</td>\n",
       "      <td>18.778041</td>\n",
       "      <td>19.734620</td>\n",
       "      <td>16.104894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-298.868347</td>\n",
       "      <td>104.327721</td>\n",
       "      <td>-21.300791</td>\n",
       "      <td>29.336489</td>\n",
       "      <td>2.966362</td>\n",
       "      <td>-3.672034</td>\n",
       "      <td>-11.293174</td>\n",
       "      <td>-2.402841</td>\n",
       "      <td>-1.284240</td>\n",
       "      <td>0.897291</td>\n",
       "      <td>...</td>\n",
       "      <td>0.480502</td>\n",
       "      <td>0.447972</td>\n",
       "      <td>0.096332</td>\n",
       "      <td>20.960629</td>\n",
       "      <td>16.082848</td>\n",
       "      <td>17.179983</td>\n",
       "      <td>16.033419</td>\n",
       "      <td>19.139572</td>\n",
       "      <td>19.313629</td>\n",
       "      <td>15.433236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0           1          2          3         4          5   \\\n",
       "0 -300.538055  102.676567 -11.533751  34.515648  3.697970   3.740451   \n",
       "1 -364.247498   91.377350   4.104960  40.972740  3.855652  11.061490   \n",
       "2 -303.546387  130.277008 -17.590935  26.203365 -7.220541   2.721291   \n",
       "3 -313.242920  110.683128 -20.646511  21.182190 -5.369611  -9.688619   \n",
       "4 -298.868347  104.327721 -21.300791  29.336489  2.966362  -3.672034   \n",
       "\n",
       "          6         7          8         9   ...        23        24  \\\n",
       "0  -7.779979 -5.030833  -1.161886 -0.948949  ...  0.428673  0.454517   \n",
       "1  -3.984364 -0.601738   4.034190 -2.416186  ...  0.511107  0.555132   \n",
       "2  -7.697184 -8.856187   1.239684 -1.230602  ...  0.466134  0.469798   \n",
       "3  -9.589869 -7.087545 -10.202554 -2.014711  ...  0.426402  0.498014   \n",
       "4 -11.293174 -2.402841  -1.284240  0.897291  ...  0.480502  0.447972   \n",
       "\n",
       "         25         26         27         28         29         30         31  \\\n",
       "0  0.127497  19.646138  14.211446  16.890984  15.460212  17.686253  19.864507   \n",
       "1  0.107820  18.037720  13.369277  16.100759  14.565921  16.484215  19.391356   \n",
       "2  0.087339  21.492927  15.680788  17.941670  17.006381  17.779363  20.308019   \n",
       "3  0.065348  21.618438  17.300488  20.150371  17.723875  18.778041  19.734620   \n",
       "4  0.096332  20.960629  16.082848  17.179983  16.033419  19.139572  19.313629   \n",
       "\n",
       "          32  \n",
       "0  14.851772  \n",
       "1  14.970760  \n",
       "2  14.854105  \n",
       "3  16.104894  \n",
       "4  15.433236  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3d0d96-da88-44a4-bfb1-a1bdfae53bca",
   "metadata": {},
   "source": [
    "### Build the Emotion Recognition Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63de68d1-3095-4871-9917-a6debb2d7ff5",
   "metadata": {},
   "source": [
    "##### ML Model (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ce706b12-0e9e-4498-bd4d-bf8e7cc51b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.28      0.41      0.33       143\n",
      "           2       0.28      0.29      0.29       163\n",
      "           3       0.41      0.27      0.33       218\n",
      "\n",
      "    accuracy                           0.31       524\n",
      "   macro avg       0.32      0.32      0.31       524\n",
      "weighted avg       0.33      0.31      0.31       524\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X = features_array  # Extracted features\n",
    "\n",
    "# Generate dummy target labels\n",
    "'''\n",
    "neutral = 1 \n",
    "sad = 2\n",
    "happy = 3\n",
    "'''\n",
    "labels = [1,2,3]\n",
    "\n",
    "# Ensure the number of labels matches the number of rows in the features array\n",
    "num_samples = features_array.shape[0]\n",
    "y = np.random.choice(labels, num_samples)  # Randomly assign one label to each sample\n",
    "\n",
    "# Assume `X` contains features and `y` contains emotion labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = rf_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf9db0a-0dec-4916-a84c-3c2a1eeb6312",
   "metadata": {},
   "source": [
    "##### Deep Learning Model (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9abd12b1-7126-4f22-9e82-ad864127098f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\komal\\anaconda3\\envs\\stt-erp\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m66/66\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3501 - loss: 3.6320 - val_accuracy: 0.3111 - val_loss: 1.1346\n",
      "Epoch 2/10\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3279 - loss: 1.2040 - val_accuracy: 0.3263 - val_loss: 1.0996\n",
      "Epoch 3/10\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3401 - loss: 1.1101 - val_accuracy: 0.2901 - val_loss: 1.1003\n",
      "Epoch 4/10\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3592 - loss: 1.1015 - val_accuracy: 0.3321 - val_loss: 1.0991\n",
      "Epoch 5/10\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3445 - loss: 1.1025 - val_accuracy: 0.4065 - val_loss: 1.0975\n",
      "Epoch 6/10\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3499 - loss: 1.1037 - val_accuracy: 0.3950 - val_loss: 1.0982\n",
      "Epoch 7/10\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3339 - loss: 1.1001 - val_accuracy: 0.2729 - val_loss: 1.0990\n",
      "Epoch 8/10\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3565 - loss: 1.1014 - val_accuracy: 0.2729 - val_loss: 1.0987\n",
      "Epoch 9/10\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3418 - loss: 1.1024 - val_accuracy: 0.2729 - val_loss: 1.0992\n",
      "Epoch 10/10\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3312 - loss: 1.1013 - val_accuracy: 0.2729 - val_loss: 1.1003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x21f2340bbe0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode labels\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "# Convert X_train and X_test to float32\n",
    "X_train = X_train.astype(np.float32).reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.astype(np.float32).reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Re-train the model\n",
    "model = Sequential([\n",
    "    Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(len(np.unique(y_train)), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8e553bed-3b10-401b-8de5-2101f2f874aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.24236641221374045\n",
      "Confusion Matrix:\n",
      " [[ 0 58 54 31]\n",
      " [ 0 60 48 55]\n",
      " [ 0 92 67 59]\n",
      " [ 0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d4d61e3f-0f49-4fe4-9539-af2ff3301e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Emotion: [2]\n",
      "WaveForm Predicted Emotion: [2]\n",
      "Predicted Emotion: [2]\n",
      "WaveForm Predicted Emotion: [2]\n",
      "Predicted Emotion: [2]\n",
      "WaveForm Predicted Emotion: [2]\n",
      "Predicted Emotion: [2]\n",
      "WaveForm Predicted Emotion: [2]\n",
      "Predicted Emotion: [2]\n",
      "WaveForm Predicted Emotion: [1]\n",
      "Predicted Emotion: [1]\n",
      "WaveForm Predicted Emotion: [2]\n",
      "Predicted Emotion: [2]\n",
      "WaveForm Predicted Emotion: [3]\n",
      "Predicted Emotion: [3]\n",
      "WaveForm Predicted Emotion: [3]\n",
      "Predicted Emotion: [3]\n",
      "WaveForm Predicted Emotion: [1]\n",
      "Predicted Emotion: [1]\n",
      "WaveForm Predicted Emotion: [1]\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "\n",
    "# Load a sample audio file from LibriSpeech\n",
    "datasets = torchaudio.datasets.LIBRISPEECH(\".\", url=\"test-clean\", download=True)\n",
    "\n",
    "key = 0\n",
    "for dataset in datasets:  \n",
    "    if ( key < 10):\n",
    "        # Access the first audio sample in the dataset\n",
    "        waveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id = dataset\n",
    "\n",
    "         # Predict emotion\n",
    "        predicted_emotion = rf_model.predict(new_features)\n",
    "        print(\"Predicted Emotion:\", predicted_emotion)\n",
    "\n",
    "         # Extract features directly from waveform\n",
    "        new_features = extract_features_from_waveform(waveform, sample_rate)\n",
    "        new_features = new_features.reshape(1, -1)  # Reshape for prediction\n",
    "        key += 1\n",
    "        \n",
    "        # Predict emotion\n",
    "        predicted_emotion = rf_model.predict(new_features)\n",
    "        print(\"WaveForm Predicted Emotion:\", predicted_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74a9e9a-154c-456c-b9db-fbd034240e55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc99ab5f-6455-4e0e-bd9f-f2cc3e6f2105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
