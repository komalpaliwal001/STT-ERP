{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3d0cb67-8563-4d65-a83f-a83dc84d0aa8",
   "metadata": {},
   "source": [
    "### Feature Extraction Using Librosa Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d16ddb-4c50-4f15-acae-edec3f938f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "\n",
    "# Function to extract features\n",
    "def extract_features(file_path):\n",
    "    audio, sample_rate = librosa.load(file_path, sr=16000)\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=13).T, axis=0)\n",
    "    chroma = np.mean(librosa.feature.chroma_stft(y=audio, sr=sample_rate).T, axis=0)\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(y=audio).T, axis=0)\n",
    "    spectral_contrast = np.mean(librosa.feature.spectral_contrast(y=audio, sr=sample_rate).T, axis=0)\n",
    "    return np.hstack([mfccs, chroma, zcr, spectral_contrast])\n",
    "\n",
    "# Load a sample audio file from LibriSpeech\n",
    "datasets = torchaudio.datasets.LIBRISPEECH(\".\", url=\"test-clean\", download=True)\n",
    "\n",
    "key = 0\n",
    "features = []\n",
    "file_path = \"librispeech_sample.wav\"\n",
    "for dataset in datasets:  \n",
    "    # Access the first audio sample in the dataset\n",
    "    waveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id = dataset\n",
    "    \n",
    "    # Save the audio to a WAV file\n",
    "    torchaudio.save(file_path, waveform, sample_rate)\n",
    "    key += 1\n",
    "\n",
    "    feature = extract_features(file_path)\n",
    "    features.append(feature) \n",
    "\n",
    "# Convert the feature list to a numpy array for further processing\n",
    "features_array = np.array(features)\n",
    "print(f\"Extracted features shape: {features_array.shape}\")\n",
    "\n",
    "# Convert np array into data freame \n",
    "features_df = pd.DataFrame(features_array)\n",
    "features_df.to_csv(\"features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ef7c8f-be5f-41bd-bb74-9b40acec923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "\n",
    "# Function to extract features from audio waveform\n",
    "def extract_features_from_waveform(waveform, sample_rate):\n",
    "    audio = waveform.numpy().flatten()  # Convert PyTorch tensor to numpy array\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=13).T, axis=0)\n",
    "    chroma = np.mean(librosa.feature.chroma_stft(y=audio, sr=sample_rate).T, axis=0)\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(y=audio).T, axis=0)\n",
    "    spectral_contrast = np.mean(librosa.feature.spectral_contrast(y=audio, sr=sample_rate).T, axis=0)\n",
    "    return np.hstack([mfccs, chroma, zcr, spectral_contrast])\n",
    "\n",
    "# Load a sample audio dataset from LibriSpeech\n",
    "datasets = torchaudio.datasets.LIBRISPEECH(\".\", url=\"test-clean\", download=True)\n",
    "\n",
    "# Extract features from first 10 utterances\n",
    "features = []\n",
    "key = 0\n",
    "for dataset in datasets:\n",
    "    # Access audio sample and its metadata\n",
    "    waveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id = dataset\n",
    "    \n",
    "    # Extract features directly from waveform\n",
    "    feature = extract_features_from_waveform(waveform, sample_rate)\n",
    "    features.append(feature)  # Append feature array for each utterance  \n",
    "\n",
    "# Convert the feature list to a numpy array for further processing\n",
    "features_array = np.array(features)\n",
    "print(f\"Extracted features shape: {features_array.shape}\")\n",
    "\n",
    "# Convert np array into data freame \n",
    "features_df = pd.DataFrame(features_array)\n",
    "features_df.to_csv(\"features.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c906ec6-be8b-4097-8822-2e6e87485d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3d0d96-da88-44a4-bfb1-a1bdfae53bca",
   "metadata": {},
   "source": [
    "### Build the Emotion Recognition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce706b12-0e9e-4498-bd4d-bf8e7cc51b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X = features_array  # Extracted features\n",
    "\n",
    "# Generate dummy target labels\n",
    "'''\n",
    "neutral = 1 \n",
    "sad = 2\n",
    "happy = 3\n",
    "'''\n",
    "labels = [0,1,2,3,4,5,6]\n",
    "\n",
    "# Ensure the number of labels matches the number of rows in the features array\n",
    "num_samples = features_array.shape[0]\n",
    "y = np.random.choice(labels, num_samples)  # Randomly assign one label to each sample\n",
    "\n",
    "# Assume `X` contains features and `y` contains emotion labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = rf_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abd12b1-7126-4f22-9e82-ad864127098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode labels\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "# Convert X_train and X_test to float32\n",
    "X_train = X_train.astype(np.float32).reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.astype(np.float32).reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Re-train the model\n",
    "model = Sequential([\n",
    "    Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(len(np.unique(y_train)), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e553bed-3b10-401b-8de5-2101f2f874aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d61e3f-0f49-4fe4-9539-af2ff3301e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "# Load a sample audio file from LibriSpeech\n",
    "datasets = torchaudio.datasets.LIBRISPEECH(\".\", url=\"test-clean\", download=True)\n",
    "\n",
    "key = 0\n",
    "for dataset in datasets:  \n",
    "    if ( key < 10):\n",
    "        # Access the first audio sample in the dataset\n",
    "        waveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id = dataset\n",
    "\n",
    "         # Extract features directly from waveform\n",
    "        new_features = extract_features_from_waveform(waveform, sample_rate)\n",
    "        new_features = new_features.reshape(1, -1)  # Reshape for prediction\n",
    "        key += 1\n",
    "        \n",
    "        # Predict emotion\n",
    "        predicted_emotion = rf_model.predict(new_features)\n",
    "        print(\"WaveForm Predicted Emotion:\", predicted_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc99ab5f-6455-4e0e-bd9f-f2cc3e6f2105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "import torch\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2ForCTC\n",
    "\n",
    "# Define a function to predict emotion from audio\n",
    "def predict_emotion(audio_path):\n",
    "    # Load the audio file\n",
    "    audio, rate = librosa.load(audio_path, sr=16000)\n",
    "    \n",
    "    # Extract features\n",
    "    inputs = feature_extractor(audio, sampling_rate=rate, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs.input_values)\n",
    "\n",
    "    # Load the pre-trained model and feature extractor\n",
    "    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"r-f/wav2vec-english-speech-emotion-recognition\")\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(\"r-f/wav2vec-english-speech-emotion-recognition\")\n",
    "    \n",
    "    # Print available labels for debugging\n",
    "    # print(\"Available labels:\", model.config.id2label)\n",
    "    \n",
    "    # Process predictions\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits.mean(dim=1), dim=-1)  # Average over sequence length\n",
    "    predicted_label = torch.argmax(predictions, dim=-1)\n",
    "\n",
    "    # Check if predicted label exists in id2label mapping\n",
    "    try:\n",
    "        emotion = model.config.id2label[predicted_label.item()]\n",
    "    except KeyError:\n",
    "        print(f\"KeyError: Predicted label {predicted_label.item()} not found in id2label.\")\n",
    "        return None\n",
    "    \n",
    "    return emotion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8064e3-c6c9-43ee-b8b8-1a015bc643d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `features` is a 2D array of extracted features and `labels` is an array of corresponding labels\n",
    "X = np.array(features_array)  # Feature matrix\n",
    "# labels = model.config.id2label.keys()\n",
    "num_samples = features_array.shape[0]\n",
    "y = np.random.randint(0, 7, num_samples) \n",
    "# y = np.random.choice(labels, num_samples)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train SVM classifier\n",
    "classifier = svm.SVC(kernel='linear')  # You can experiment with different kernels\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd342b23-fa99-40d2-8e66-fa976a257b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample audio file from LibriSpeech\n",
    "datasets = torchaudio.datasets.LIBRISPEECH(\".\", url=\"test-clean\", download=True)\n",
    "\n",
    "\n",
    "file_path = \"librispeech_sample.wav\"\n",
    "key = 0\n",
    "# Loop through the dataset and process the first 10 samples\n",
    "for dataset in datasets:  \n",
    "     if ( key < 10):\n",
    "        # Access the first audio sample in the dataset\n",
    "        waveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id = dataset\n",
    "        \n",
    "        # Save the audio to a WAV file\n",
    "        torchaudio.save(file_path, waveform, sample_rate)\n",
    "        \n",
    "        # Predict emotion for the saved audio file\n",
    "        emotion = predict_emotion(file_path)\n",
    "        key += 1\n",
    "        if emotion is not None:\n",
    "            print(f\"Predicted emotion for sample {key}: {emotion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93614eee-6d70-4f0a-9afc-fc0655a8078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d242b6f-7a1a-4fa9-883a-edd1cb5c4c4a",
   "metadata": {},
   "source": [
    "@TODO so what my next step will be what ever the model predict i will assign to a category which will near to the any category that way i can classifiy any model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d88977f-8bf6-4d18-9cb5-e5830fb319b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torchaudio\n",
    "\n",
    "# Load the pre-trained speech embedding model\n",
    "model_url = \"https://tfhub.dev/google/speech_embedding/1\"\n",
    "model = hub.load(model_url)\n",
    "\n",
    "# Define a function to extract embeddings\n",
    "def extract_embeddings(audio_path, max_length=123):\n",
    "    # Load the audio file\n",
    "    audio, rate = librosa.load(audio_path, sr=16000)\n",
    "    \n",
    "    # Ensure the input has the correct shape (batch size, time)\n",
    "    audio_tensor = tf.constant(audio[np.newaxis, :], dtype=tf.float32)\n",
    "\n",
    "    # Use the model's default signature to extract embeddings\n",
    "    if \"default\" in model.signatures:\n",
    "        embeddings = model.signatures[\"default\"](audio_tensor)[\"default\"]\n",
    "    else:\n",
    "        raise ValueError(\"The model does not have a callable default signature.\")\n",
    "    \n",
    "    # Convert embeddings to numpy array and reshape\n",
    "    embeddings = embeddings.numpy().squeeze()  # Remove batch and channel dimensions\n",
    "    if len(embeddings.shape) == 3:  # If shape is (123, 1, 96), remove extra dimension\n",
    "        embeddings = embeddings[:, 0, :]\n",
    "    \n",
    "    # Pad or truncate embeddings to ensure uniform shape\n",
    "    if embeddings.shape[0] < max_length:\n",
    "        padding = np.zeros((max_length - embeddings.shape[0], embeddings.shape[1]))\n",
    "        embeddings = np.vstack([embeddings, padding])  # Pad\n",
    "    elif embeddings.shape[0] > max_length:\n",
    "        embeddings = embeddings[:max_length, :]  # Truncate\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Load a sample audio dataset from LibriSpeech\n",
    "datasets = torchaudio.datasets.LIBRISPEECH(\".\", url=\"test-clean\", download=True)\n",
    "\n",
    "# File path for temporary storage\n",
    "file_path = \"librispeech_sample.wav\"\n",
    "\n",
    "features = []  # List to store extracted features\n",
    "key = 0  # Counter for processed samples\n",
    "max_length = 123  # Maximum length for embeddings (adjust based on your data)\n",
    "\n",
    "# Loop through the dataset and process the first 10 samples\n",
    "for dataset in datasets:\n",
    "    if key < 10:\n",
    "        # Access the first audio sample in the dataset\n",
    "        waveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id = dataset\n",
    "\n",
    "        # Save the audio to a WAV file for processing\n",
    "        torchaudio.save(file_path, waveform, sample_rate)\n",
    "\n",
    "        # Extract embeddings using the pre-trained model\n",
    "        embeddings = extract_embeddings(file_path, max_length=max_length)\n",
    "        features.append(embeddings)  # Store the embeddings\n",
    "\n",
    "        key += 1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Convert the features list to a 3D NumPy array\n",
    "features_array = np.array(features)\n",
    "print(f\"Extracted features shape: {features_array.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f805cee1-c8ad-4007-88cf-415ab537d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b84196-962f-4541-93b5-29329e772542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Extract the actual embedding values\n",
    "extracted_embeddings = embeddings['default'].numpy()  # Shape: (1, 123, 1, 96)\n",
    "\n",
    "# Reshape embeddings to be 2D: (samples, features)\n",
    "flattened_embeddings = extracted_embeddings.reshape(extracted_embeddings.shape[1], -1)  # Shape: (123, 96)\n",
    "\n",
    "# Generate random labels for the embeddings (replace this with actual labels in a real dataset)\n",
    "num_samples = flattened_embeddings.shape[0]\n",
    "labels = np.random.randint(0, 7, size=num_samples)  # 7 is the number of classes\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(flattened_embeddings, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train an SVM classifier\n",
    "classifier = SVC(kernel='linear')  # You can experiment with different kernels\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6856a4e9-4fd4-43fa-a906-3e250c590126",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
